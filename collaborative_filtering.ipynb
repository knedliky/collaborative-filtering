{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N3xQa6VlNAOe"
   },
   "source": [
    "# Collaborative Filtering\n",
    "Missing values in datsets are a challenging aspect in Data Science Projects. In recommender systems, collaborative filtering is used to provide recommendations to users, based on similarities between indvidual users, as well as individual items. Currently, there are two standard methods of collaborative filterings used in typical recommendation systems - model-based and memory based approaches. Model-based approaches include using the K Nearest Neighbour algorithm (KNN) to solve for the missing value, where memory-based approaches tackle this problem by using either user-item filtering, or item-item filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "0mG3n0qRNTSA",
    "outputId": "cb0eadbd-b8a8-41b1-e3b4-7f44b47a8804"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3</td>\n",
       "      <td>881250949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "      <td>891717742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>377</td>\n",
       "      <td>1</td>\n",
       "      <td>878887116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>880606923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>346</td>\n",
       "      <td>1</td>\n",
       "      <td>886397596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating  timestamp\n",
       "0      196      242       3  881250949\n",
       "1      186      302       3  891717742\n",
       "2       22      377       1  878887116\n",
       "3      244       51       2  880606923\n",
       "4      166      346       1  886397596"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load MovieLens 100K dataset into a dataframe of pandas\n",
    "names = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "df = pd.read_csv('ml-100k/u.data', sep='\\t', names=names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "hFJT0QWudp1P"
   },
   "outputs": [],
   "source": [
    "# Select 500 most active users and 500 most active items from the dataset\n",
    "n_most_active_users = 500\n",
    "n_most_active_items = 500\n",
    "\n",
    "user_ids = df.groupby('user_id').count().sort_values(by='rating', ascending=False).head(n_most_active_users).index\n",
    "item_ids = df.groupby('item_id').count().sort_values(by='rating', ascending=False).head(n_most_active_items).index\n",
    "df = df[(df['user_id'].isin(user_ids)) & (df['item_id'].isin(item_ids))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "r012fu0jJkJc"
   },
   "outputs": [],
   "source": [
    "# Map new internal ID for items\n",
    "i_ids = df['item_id'].unique().tolist()\n",
    "item_dict = dict(zip(i_ids, [i for i in range(len(i_ids))]))\n",
    "df['item_id'] = df['item_id'].map(item_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vZ3rlC7jO6WJ"
   },
   "source": [
    "# Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EwS00lvHO-ca",
    "outputId": "f9f3a031-b4e7-439e-c3e3-c165b2286d19"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-fb140fbf3b12>:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df['user_id'] = train_df['user_id'].map(user_dict)\n",
      "<ipython-input-4-fb140fbf3b12>:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  remain_df['user_id'] = remain_df['user_id'].map(user_dict)\n"
     ]
    }
   ],
   "source": [
    "# The number of training users and active users\n",
    "n_training_users = 300\n",
    "n_active_users = n_most_active_users - n_training_users\n",
    "\n",
    "# The number of GIVEN ratings for active users\n",
    "GIVEN = 20\n",
    "\n",
    "# Randomly select users from the most active users as training set\n",
    "random_uids = np.random.choice(df.user_id.unique(), n_training_users, replace=False)\n",
    "train_df = df[df['user_id'].isin(random_uids)]\n",
    "# Map new internal ID for all users in the training set\n",
    "u_ids = train_df['user_id'].unique().tolist()\n",
    "user_dict = dict(zip(u_ids, [i for i in range(len(u_ids))]))\n",
    "train_df['user_id'] = train_df['user_id'].map(user_dict)\n",
    "\n",
    "# The rest of users are active users for testing\n",
    "remain_df = df[~df['user_id'].isin(random_uids)]\n",
    "# Map new internal ID for all active users\n",
    "u_ids = remain_df['user_id'].unique().tolist()\n",
    "user_dict = dict(zip(u_ids, [i for i in range(len(u_ids))]))\n",
    "remain_df['user_id'] = remain_df['user_id'].map(user_dict)\n",
    "\n",
    "# Randomly select GIVEN ratings for active users\n",
    "active_df = remain_df.groupby('user_id').sample(n=GIVEN, random_state=1024)\n",
    "\n",
    "test_df = remain_df[~remain_df.index.isin(active_df.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c-ke62G3jiYb",
    "outputId": "1c135984-edb4-4f2b-fc73-9225d86a0c38"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(item_id  0    1    2    3    4    5    6    7    8    9    ...  490  491  492  \\\n",
       " user_id                                                    ...                  \n",
       " 0        0.0  2.0  0.0  4.0  0.0  4.0  4.0  0.0  0.0  2.0  ...  0.0  4.0  4.0   \n",
       " 1        0.0  0.0  4.0  4.0  4.0  0.0  0.0  4.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       " 2        4.0  0.0  5.0  0.0  0.0  3.0  4.0  2.0  0.0  2.0  ...  0.0  2.0  0.0   \n",
       " 3        4.0  0.0  5.0  0.0  1.0  0.0  3.0  2.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       " 4        3.0  0.0  4.0  0.0  0.0  3.0  2.0  2.0  0.0  5.0  ...  0.0  4.0  0.0   \n",
       " ...      ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       " 295      4.0  0.0  0.0  0.0  0.0  0.0  0.0  4.0  0.0  0.0  ...  0.0  0.0  4.0   \n",
       " 296      0.0  0.0  5.0  0.0  0.0  0.0  4.0  0.0  0.0  4.0  ...  0.0  0.0  0.0   \n",
       " 297      0.0  0.0  5.0  4.0  0.0  1.0  0.0  0.0  0.0  1.0  ...  0.0  0.0  0.0   \n",
       " 298      0.0  0.0  0.0  3.0  0.0  0.0  0.0  4.0  0.0  0.0  ...  0.0  0.0  5.0   \n",
       " 299      0.0  0.0  0.0  0.0  0.0  0.0  0.0  3.0  3.0  0.0  ...  4.0  0.0  0.0   \n",
       " \n",
       " item_id  493  494  495  496  497  498  499  \n",
       " user_id                                     \n",
       " 0        0.0  3.0  3.0  0.0  0.0  0.0  0.0  \n",
       " 1        0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 2        0.0  0.0  0.0  0.0  4.0  0.0  0.0  \n",
       " 3        4.0  1.0  0.0  0.0  0.0  0.0  2.0  \n",
       " 4        0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " ...      ...  ...  ...  ...  ...  ...  ...  \n",
       " 295      0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 296      0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 297      0.0  0.0  0.0  0.0  0.0  1.0  0.0  \n",
       " 298      0.0  0.0  3.0  0.0  0.0  0.0  0.0  \n",
       " 299      5.0  0.0  0.0  3.0  0.0  0.0  0.0  \n",
       " \n",
       " [300 rows x 500 columns],\n",
       " item_id  0    1    2    3    4    5    6    7    8    9    ...  490  491  492  \\\n",
       " user_id                                                    ...                  \n",
       " 0        3.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       " 1        4.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       " 2        0.0  0.0  0.0  0.0  5.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       " 3        0.0  4.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       " 4        0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       " ...      ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       " 195      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       " 196      0.0  0.0  0.0  0.0  0.0  0.0  0.0  5.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       " 197      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       " 198      0.0  0.0  0.0  5.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       " 199      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  4.0  0.0   \n",
       " \n",
       " item_id  493  494  495  496  497  498  499  \n",
       " user_id                                     \n",
       " 0        0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 1        0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 2        0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 3        0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 4        0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " ...      ...  ...  ...  ...  ...  ...  ...  \n",
       " 195      0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 196      0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 197      0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 198      0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 199      0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " \n",
       " [200 rows x 500 columns],\n",
       " item_id  0    1    2    3    4    5    6    7    8    9    ...  490  491  492  \\\n",
       " user_id                                                    ...                  \n",
       " 0        0.0  0.0  0.0  0.0  0.0  0.0  0.0  4.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       " 1        0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       " 2        0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  4.0  ...  0.0  0.0  0.0   \n",
       " 3        0.0  0.0  0.0  0.0  0.0  0.0  3.0  0.0  0.0  4.0  ...  0.0  0.0  3.0   \n",
       " 4        0.0  4.0  4.0  3.0  3.0  3.0  0.0  5.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       " ...      ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       " 195      4.0  0.0  4.0  0.0  0.0  4.0  3.0  4.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       " 196      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  4.0  5.0  ...  0.0  0.0  4.0   \n",
       " 197      4.0  0.0  3.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       " 198      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       " 199      0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  3.0  0.0  0.0   \n",
       " \n",
       " item_id  493  494  495  496  497  498  499  \n",
       " user_id                                     \n",
       " 0        0.0  0.0  0.0  4.0  0.0  3.0  0.0  \n",
       " 1        0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 2        0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 3        0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 4        0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " ...      ...  ...  ...  ...  ...  ...  ...  \n",
       " 195      0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 196      0.0  4.0  2.0  3.0  0.0  0.0  0.0  \n",
       " 197      0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 198      0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 199      0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " \n",
       " [200 rows x 500 columns])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the format of datasets to matrices\n",
    "df_zeros = pd.DataFrame({'user_id': np.tile(np.arange(0, n_training_users), n_most_active_items), 'item_id': np.repeat(np.arange(0, n_most_active_items), n_training_users), 'rating': 0})\n",
    "train_ds = df_zeros.merge(train_df, how='left', on=['user_id', 'item_id']).fillna(0.).pivot_table(values='rating_y', index='user_id', columns='item_id')\n",
    "\n",
    "df_zeros = pd.DataFrame({'user_id': np.tile(np.arange(0, n_active_users), n_most_active_items), 'item_id': np.repeat(np.arange(0, n_most_active_items), n_active_users), 'rating': 0})\n",
    "active_ds = df_zeros.merge(active_df, how='left', on=['user_id', 'item_id']).fillna(0.).pivot_table(values='rating_y', index='user_id', columns='item_id')\n",
    "test_ds = df_zeros.merge(test_df, how='left', on=['user_id', 'item_id']).fillna(0.).pivot_table(values='rating_y', index='user_id', columns='item_id')\n",
    "\n",
    "train_ds, active_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "yElYv2TDKKGu"
   },
   "outputs": [],
   "source": [
    "# Predicting All Missing Data in training set\n",
    "imputed_train_ds = train_ds.values.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iy4FurbHD4dt"
   },
   "source": [
    "# Predicting the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "zYh1bVd0ncz3",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "LAMBDA = 0.7    # λ\n",
    "GAMMA = 10      # γ\n",
    "DELTA = 10      # δ\n",
    "ITA = 0.7       # η\n",
    "THETA = 0.7     # θ\n",
    "EPSILON = 1e-9\n",
    "\n",
    "def nonzero_mean(matrix):\n",
    "    '''\n",
    "    A function to return a single vector of average values, ignoring values of 0\n",
    "    '''\n",
    "    return np.true_divide(matrix.sum(1), (matrix!=0).sum(1))\n",
    "\n",
    "def row_similarity(row_a, row_b):\n",
    "    '''\n",
    "    A function to return the Pearson Correlation Coefficient of two rows in a given matrix\n",
    "    '''\n",
    "    \n",
    "    # Filter each row by masking based on the other\n",
    "    mask = row_a * row_b > 0\n",
    "    \n",
    "    if mask.sum() == 0:\n",
    "        return 0\n",
    "    \n",
    "    # The rating vaulues/ intersection of each row\n",
    "    intersection_a = row_a[mask]\n",
    "    intersection_b = row_b[mask]\n",
    "    \n",
    "    # The average of each set of intersecting values\n",
    "    mean_a = intersection_a.mean()\n",
    "    mean_b = intersection_b.mean()\n",
    "    \n",
    "    # The normalised value for each set of values\n",
    "    norm_a = intersection_a - mean_a\n",
    "    norm_b = intersection_b - mean_b\n",
    "    \n",
    "    numerator = np.sum(norm_a * norm_b)\n",
    "    denominator = np.sqrt(np.sum(np.square(norm_a))) * np.sqrt(np.sum(np.square(norm_b)))\n",
    "    \n",
    "    if denominator == 0:\n",
    "        return 0\n",
    "    \n",
    "    return numerator/ denominator\n",
    "\n",
    "def weighted_row_similarity(row_a, row_b, weight):\n",
    "    '''\n",
    "    A function to return the weighted Pearson Correlation Coefficient by reducing the impact of\n",
    "    highly similar items or users\n",
    "    '''\n",
    "    return min(np.sum(row_a * row_b > 0), weight)/ weight * row_similarity(row_a, row_b)\n",
    "\n",
    "def similarity_matrices(matrix):\n",
    "    '''\n",
    "    A function to return the Pearson Correlation Matrices for items and users, as well as the lists of \n",
    "    each users similar neighbours, and each items similar neigbours\n",
    "    '''\n",
    "    # Setting the sizes of the matrices based on the number of rows and columns of the input matrix\n",
    "    user_similarity_matrix = np.zeros((len(matrix), len(matrix)))\n",
    "    item_similarity_matrix = np.zeros((len(matrix.T), len(matrix.T)))\n",
    "\n",
    "    # Instantiating a list of empty lists for each user and item in the given matrix\n",
    "    similar_users = [[] for user in range(len(matrix))]\n",
    "    similar_items = [[] for item in range(len(matrix.T))]\n",
    "    \n",
    "    # Iterating over each value of the empty matrices and filling them in, allocating the\n",
    "    # user or item to their respective neighbour if they exceed the threshhold\n",
    "    for (u, a), value in np.ndenumerate(user_similarity_matrix):\n",
    "        user_similarity_matrix[u][a] = weighted_row_similarity(matrix[u], matrix[a], GAMMA)\n",
    "        \n",
    "        # Add the user_a to a list of user_u's similar neighbours if above the threshhold\n",
    "        if a not in similar_users[u] and user_similarity_matrix[u][a] > ITA and u != a:\n",
    "            similar_users[u].append(a)\n",
    "        \n",
    "    for (i, j), value in np.ndenumerate(item_similarity_matrix):\n",
    "        item_similarity_matrix[i][j] = weighted_row_similarity(matrix.T[i], matrix.T[j], DELTA)\n",
    "        \n",
    "        # Add the item_j to a list of item_i's similar neighbours if above the threshhold\n",
    "        if j not in similar_items[i] and item_similarity_matrix[i][j] > THETA and i != j:\n",
    "            similar_items[i].append(j)\n",
    "    \n",
    "    return user_similarity_matrix, item_similarity_matrix, similar_users, similar_items\n",
    "    \n",
    "def predict(input_matrix):\n",
    "    '''\n",
    "    A function to predict the missing values of a given a sparsely populated user/ item matrix \n",
    "    '''    \n",
    "    # Create a copy of each of the matrices required for better performance\n",
    "    user_similarity_matrix, item_similarity_matrix, similar_user_ids, similar_item_ids = similarity_matrices(input_matrix)\n",
    "    \n",
    "    # Creating a copy of the input matrix \n",
    "    matrix = np.zeros_like(input_matrix)\n",
    "    \n",
    "    # Calculating the overall user and item averages for use in the final prediction\n",
    "    overall_user_mean = nonzero_mean(input_matrix)\n",
    "    overall_item_mean = nonzero_mean(input_matrix.T)\n",
    "    \n",
    "    for (user, item), value in np.ndenumerate(input_matrix):\n",
    "        # Add the existing rating to the new matrix\n",
    "        if value > 0:\n",
    "            matrix[user][item] = value\n",
    "            continue\n",
    "        \n",
    "        # Instantiating and resetting the numerator and denominator values, and the similar users and items for the prediction\n",
    "        numerator_user = numerator_item = denominator_user = denominator_item = 0\n",
    "        \n",
    "        # Set the equations up for both user based and item based collaborative filtering\n",
    "        similar_users = input_matrix[similar_user_ids[user]]\n",
    "        similar_items = input_matrix[:,similar_item_ids[item]]\n",
    "            \n",
    "        # Calculate the mean vector for similar users and items, transposing the items\n",
    "        user_mean = nonzero_mean(similar_users)\n",
    "        item_mean = nonzero_mean(similar_items.T)\n",
    "        \n",
    "         # Selecting the similar items and users for this particular cell\n",
    "        user_similarity = user_similarity_matrix[user, similar_user_ids[user]]\n",
    "        item_similarity = item_similarity_matrix[item, similar_item_ids[item]]\n",
    "        \n",
    "        # Create a mask to only show similar users who have rated the item, or similar items that have been rated by the user\n",
    "        users_mask = similar_users[:, item] > 0\n",
    "        items_mask = similar_items[user, :] > 0\n",
    "        \n",
    "        # If the masks equate to 0, then there is no intersection, and the matrix value is 0\n",
    "        if sum(users_mask) == 0 and sum(items_mask) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Calculate the values for user based collaborative filtering\n",
    "        if sum(users_mask) > 0:\n",
    "            numerator_user = np.sum(user_similarity[users_mask] * (similar_users[users_mask, item] - user_mean[users_mask]))\n",
    "            denominator_user = np.sum(user_similarity[users_mask])\n",
    "\n",
    "        # Calculate the values for item based collaborative filtering\n",
    "        if sum(items_mask) > 0:\n",
    "            numerator_item = np.sum(item_similarity[items_mask] * (similar_items[user, items_mask] - item_mean[items_mask]))\n",
    "            denominator_item = np.sum(item_similarity[items_mask])\n",
    "        \n",
    "        # Logic for selecting which algoritm to use to predict values \n",
    "        if sum(users_mask) > 0 and sum(items_mask) > 0:\n",
    "            prediction = LAMBDA * (overall_user_mean[user] + numerator_user/ denominator_user) + (1 - LAMBDA) * (overall_item_mean[item] + numerator_item/ denominator_item)\n",
    "            matrix[user][item] = np.clip(prediction, 0, 5)\n",
    "        \n",
    "        elif sum(users_mask) > 0 and sum(items_mask) == 0:\n",
    "            prediction = overall_user_mean[user] + numerator_user/ denominator_user\n",
    "            matrix[user][item] = np.clip(prediction, 0, 5)        \n",
    "        \n",
    "        elif sum(users_mask) == 0 and sum(items_mask) > 0:\n",
    "            prediction = overall_item_mean[item] + numerator_item/ denominator_item\n",
    "            matrix[user][item] = np.clip(prediction, 0, 5)\n",
    "            continue\n",
    "        \n",
    "    return matrix\n",
    "\n",
    "# Finally, run the function in order to predict the missing values in place\n",
    "imputed_train_ds = predict(imputed_train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GbOfVWTV_Aij"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Txs8YjwTzuSP"
   },
   "source": [
    "### Compute Pearson Correlation Coefficient of All Pairs of Items between active set and imputed training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 439
    },
    "id": "KoOgX_axKKGw",
    "outputId": "37a7adbd-e0d6-4375-e28c-3940977896f2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>490</th>\n",
       "      <th>491</th>\n",
       "      <th>492</th>\n",
       "      <th>493</th>\n",
       "      <th>494</th>\n",
       "      <th>495</th>\n",
       "      <th>496</th>\n",
       "      <th>497</th>\n",
       "      <th>498</th>\n",
       "      <th>499</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.683260</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.513566</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.083117</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.907326</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.168048</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.55073</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.838566</td>\n",
       "      <td>2.521215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.858893</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.109316</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.184618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.361228</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.193649</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.358137</td>\n",
       "      <td>1.984634</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.044276</td>\n",
       "      <td>2.386998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.559591</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.761117</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.590450</td>\n",
       "      <td>3.761117</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.363478</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.879455</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>3.810811</td>\n",
       "      <td>1.471592</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.599157</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.628593</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.317805</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.245466</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.55073</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.180002</td>\n",
       "      <td>2.793568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.863522</td>\n",
       "      <td>1.863522</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.207495</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.451320</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.396787</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.980317</td>\n",
       "      <td>3.313727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.747825</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.218850</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.340000</td>\n",
       "      <td>3.662193</td>\n",
       "      <td>3.114576</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.55073</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.819369</td>\n",
       "      <td>3.174668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>3.492409</td>\n",
       "      <td>1.854578</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.469308</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.162393</td>\n",
       "      <td>2.579097</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.317805</td>\n",
       "      <td>1.803667</td>\n",
       "      <td>2.215675</td>\n",
       "      <td>2.567770</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.55073</td>\n",
       "      <td>3.162393</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.074521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>4.224932</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.334402</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.224932</td>\n",
       "      <td>...</td>\n",
       "      <td>1.224932</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.983554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>3.878005</td>\n",
       "      <td>4.717391</td>\n",
       "      <td>4.650990</td>\n",
       "      <td>3.872661</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.728184</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.134416</td>\n",
       "      <td>...</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.827913</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>3.728184</td>\n",
       "      <td>3.489042</td>\n",
       "      <td>2.916127</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6    \\\n",
       "0    4.683260  2.000000  0.000000  4.000000  3.513566  4.000000  4.000000   \n",
       "1    0.000000  0.000000  4.000000  4.000000  4.000000  0.000000  0.000000   \n",
       "2    4.000000  3.361228  5.000000  0.000000  3.193649  3.000000  4.000000   \n",
       "3    4.000000  3.559591  5.000000  2.761117  1.000000  0.000000  3.000000   \n",
       "4    3.000000  3.599157  4.000000  0.000000  2.628593  3.000000  2.000000   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "295  4.000000  0.000000  4.863522  1.863522  0.000000  2.207495  0.000000   \n",
       "296  5.000000  3.747825  5.000000  5.000000  4.218850  0.000000  4.000000   \n",
       "297  3.492409  1.854578  5.000000  4.000000  2.469308  1.000000  3.162393   \n",
       "298  4.224932  0.000000  0.000000  3.000000  3.334402  0.000000  0.000000   \n",
       "299  3.878005  4.717391  4.650990  3.872661  0.000000  0.000000  2.728184   \n",
       "\n",
       "          7         8         9    ...       490       491       492  \\\n",
       "0    0.000000  3.083117  2.000000  ...  1.907326  4.000000  4.000000   \n",
       "1    4.000000  3.858893  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "2    2.000000  0.000000  2.000000  ...  0.000000  2.000000  0.000000   \n",
       "3    2.000000  3.590450  3.761117  ...  0.000000  0.000000  4.363478   \n",
       "4    2.000000  0.000000  5.000000  ...  2.317805  4.000000  0.000000   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "295  4.000000  0.000000  0.000000  ...  0.000000  3.451320  4.000000   \n",
       "296  0.000000  0.000000  4.000000  ...  0.000000  0.000000  3.340000   \n",
       "297  2.579097  0.000000  1.000000  ...  1.317805  1.803667  2.215675   \n",
       "298  4.000000  0.000000  3.224932  ...  1.224932  0.000000  5.000000   \n",
       "299  3.000000  3.000000  3.134416  ...  4.000000  0.000000  0.000000   \n",
       "\n",
       "          493       494       495      496       497       498       499  \n",
       "0    4.168048  3.000000  3.000000  1.55073  0.000000  2.838566  2.521215  \n",
       "1    0.000000  3.109316  0.000000  0.00000  0.000000  0.000000  3.184618  \n",
       "2    0.000000  3.358137  1.984634  0.00000  4.000000  3.044276  2.386998  \n",
       "3    4.000000  1.000000  0.879455  0.00000  3.810811  1.471592  2.000000  \n",
       "4    4.245466  0.000000  0.000000  2.55073  0.000000  2.180002  2.793568  \n",
       "..        ...       ...       ...      ...       ...       ...       ...  \n",
       "295  0.000000  0.000000  2.396787  0.00000  0.000000  2.980317  3.313727  \n",
       "296  3.662193  3.114576  0.000000  1.55073  0.000000  2.819369  3.174668  \n",
       "297  2.567770  0.000000  0.000000  1.55073  3.162393  1.000000  2.074521  \n",
       "298  0.000000  0.000000  3.000000  0.00000  0.000000  0.000000  1.983554  \n",
       "299  5.000000  0.000000  2.827913  3.00000  3.728184  3.489042  2.916127  \n",
       "\n",
       "[300 rows x 500 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputed_train_ds = pd.DataFrame(imputed_train_ds)\n",
    "imputed_train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eq0uq1aHzu11",
    "outputId": "f6214a46-d63e-4fdc-e7dd-ccaed23b237d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.13607979, -0.25727108, -0.30892898, ...,  0.22800604,\n",
       "         0.55783056, -0.22405739],\n",
       "       [ 0.56538838,  0.34868768,  0.48257192, ...,  0.17102121,\n",
       "        -0.25235698,  0.29905257],\n",
       "       [ 0.42671442,  0.52765172,  0.62197315, ...,  0.11340032,\n",
       "         0.19761727, -0.06683286],\n",
       "       ...,\n",
       "       [ 0.51264913,  0.14779686,  0.19909483, ..., -0.0585631 ,\n",
       "         0.41779851, -0.22511708],\n",
       "       [ 0.11625085,  0.65192938, -0.34922572, ...,  0.21399922,\n",
       "        -0.44595879,  0.04742908],\n",
       "       [ 0.44919548,  0.45357547,  0.29628982, ...,  0.58505563,\n",
       "         0.3418939 ,  0.35291927]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_user_pearson_corr = np.zeros((active_ds.shape[0], train_ds.shape[0]))\n",
    "\n",
    "# Compute Pearson Correlation Coefficient of All Pairs of Users between active set and imputed training set\n",
    "for i, user_i_vec in enumerate(active_ds.values):\n",
    "    for j, user_j_vec in enumerate(imputed_train_ds.values):\n",
    "        \n",
    "        # ratings corated by the current pair od users\n",
    "        mask_i = user_i_vec > 0\n",
    "        mask_j = user_j_vec > 0\n",
    "\n",
    "        # corrated item index, skip if there are no corrated ratings\n",
    "        corrated_index = np.intersect1d(np.where(mask_i), np.where(mask_j))\n",
    "        if len(corrated_index) == 0:\n",
    "            continue\n",
    "\n",
    "        # average value of user_i_vec and user_j_vec\n",
    "        mean_user_i = np.sum(user_i_vec) / (np.sum(np.clip(user_i_vec, 0, 1)) + EPSILON)\n",
    "        mean_user_j = np.sum(user_j_vec) / (np.sum(np.clip(user_j_vec, 0, 1)) + EPSILON)\n",
    "\n",
    "        # compute pearson corr\n",
    "        user_i_sub_mean = user_i_vec[corrated_index] - mean_user_i\n",
    "        user_j_sub_mean = user_j_vec[corrated_index] - mean_user_j\n",
    "\n",
    "        r_ui_sub_r_i_sq = np.square(user_i_sub_mean)\n",
    "        r_uj_sub_r_j_sq = np.square(user_j_sub_mean)\n",
    "\n",
    "        r_ui_sum_sqrt = np.sqrt(np.sum(r_ui_sub_r_i_sq))\n",
    "        r_uj_sum_sqrt = np.sqrt(np.sum(r_uj_sub_r_j_sq))\n",
    "\n",
    "        sim = np.sum(user_i_sub_mean * user_j_sub_mean) / (r_ui_sum_sqrt * r_uj_sum_sqrt + EPSILON)\n",
    "\n",
    "        # significance weighting\n",
    "        weighted_sim = (min(len(corrated_index), GAMMA) / GAMMA) * sim\n",
    "\n",
    "        active_user_pearson_corr[i][j] = weighted_sim\n",
    "\n",
    "active_user_pearson_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ewTnN9kNb8Ys"
   },
   "source": [
    "## Predict Ratings of Testing Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L4ERndYXb8Ys",
    "outputId": "12607670-af61-403a-e4ce-f5e874bf8386"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 2.84874172,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [3.79504562, 0.        , 4.47525958, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 4.04221674, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = 10\n",
    "\n",
    "test_ds_pred = np.zeros_like(test_ds.values)\n",
    "\n",
    "for (i, j), rating in np.ndenumerate(test_ds.values):\n",
    "\n",
    "    if rating > 0:\n",
    "\n",
    "        sim_user_ids = np.argsort(active_user_pearson_corr[i])[-1:-(K + 1):-1]\n",
    "\n",
    "        #==================user-based==================#\n",
    "        # the coefficient values of similar users\n",
    "        sim_val = active_user_pearson_corr[i][sim_user_ids]\n",
    "\n",
    "        # the average value of the current user's ratings\n",
    "        sim_users = imputed_train_ds.values[sim_user_ids]\n",
    "        user_mean = np.sum(active_ds.values[i]) / (np.sum(np.clip(active_ds.values[i], 0, 1)) + EPSILON)\n",
    "        sim_user_mean = np.sum(sim_users, axis=1) / (np.sum(np.clip(sim_users, 0, 1), axis=1) + EPSILON)\n",
    "\n",
    "        # select the users who rated item j\n",
    "        mask_rated_j = sim_users[:, j] > 0\n",
    "        \n",
    "        # sim(u, v) * (r_vj - mean_v)\n",
    "        sim_r_sum_mean = sim_val[mask_rated_j] * (sim_users[mask_rated_j, j] - sim_user_mean[mask_rated_j])\n",
    "        \n",
    "        user_based_pred = user_mean + np.sum(sim_r_sum_mean) / (np.sum(sim_val[mask_rated_j]) + EPSILON)\n",
    "        user_based_pred = np.clip(user_based_pred, 0, 5)\n",
    "\n",
    "        test_ds_pred[i][j] = user_based_pred\n",
    "        \n",
    "test_ds_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oUTn4kSFb8ZA"
   },
   "source": [
    "## Compute MAE and RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7JCVmexyb8ZA",
    "outputId": "6f650170-e9a6-482a-b695-5fa474d964b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.7962134693937437, RMSE: 1.0178289876151287\n"
     ]
    }
   ],
   "source": [
    "# MAE\n",
    "MAE = np.sum(np.abs(test_ds_pred - test_ds.values)) / np.sum(np.clip(test_ds.values, 0, 1))\n",
    "\n",
    "# RMSE\n",
    "RMSE = np.sqrt(np.sum(np.square(test_ds_pred - test_ds.values)) / np.sum(np.clip(test_ds.values, 0, 1)))\n",
    "\n",
    "print(\"MAE: {}, RMSE: {}\" .format(MAE, RMSE))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "assignment3_framework.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
